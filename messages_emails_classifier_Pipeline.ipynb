{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jZxeQ8FGnnD",
        "outputId": "bfba3d88-7669-4c28-f52a-c4fb109b535d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.models import load_model\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the head of eamil\n",
        "def remove_header(email):\n",
        "    \"\"\"remove the header from an email\"\"\"\n",
        "    return email[email.index('\\n\\n'):]\n",
        "\n",
        "\n",
        "def remove_html_tags(input):\n",
        "    soup = BeautifulSoup(input, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "# replace URLs with oussama word and emails with boussaid\n",
        "def remove_hyperlink(word):\n",
        "    regex_links = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    word_without_links =  re.sub(regex_links,\"oussama\", word)\n",
        "    regex_email = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "    return re.sub(regex_email,\"boussaid\", word_without_links)\n",
        "\n",
        "\n",
        "# make word in lower case\n",
        "def to_lower(word):\n",
        "    return word.lower()\n",
        "\n",
        "\n",
        "# remove whitespaces\n",
        "def remove_whitespace(word):\n",
        "    return word.strip()\n",
        "\n",
        "\n",
        "def remove_digits(word):\n",
        "  '''This function removes all the numbers'''\n",
        "  return re.sub('\\d+', '', word)\n",
        "\n",
        "def remove_underscores(word):\n",
        "  '''This function removes all the underscores'''\n",
        "  return re.sub(r'_', '', word)\n",
        "\n",
        "\n",
        "def remove_special_characters(word):\n",
        "  '''This function removes all the special characters'''\n",
        "  return re.sub('\\W', ' ', word)\n",
        "\n",
        "# remove stop words\n",
        "stopwords_english = stopwords.words('english')\n",
        "def remove_stopwords(word,stopword_list=stopwords_english):\n",
        "  '''This function removes the stop words'''\n",
        "  word_list = word.split(\" \")\n",
        "  cleaned_txt = [w for w in word_list if not w in stopword_list]\n",
        "  cleaned_string = \" \".join(cleaned_txt)\n",
        "\n",
        "  return cleaned_string\n",
        "\n",
        "def EmailsPreprocessor(sentence):\n",
        "\n",
        "    Preprocessor_utils = [remove_header,\n",
        "                      remove_html_tags,\n",
        "                      to_lower,\n",
        "                      remove_hyperlink,\n",
        "                      remove_whitespace,\n",
        "                      remove_digits,\n",
        "                      remove_underscores,\n",
        "                      remove_stopwords,\n",
        "                      remove_special_characters]\n",
        "\n",
        "    for tool in Preprocessor_utils:\n",
        "        sentence = tool(sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def Tokenizer_email(email):\n",
        "  max_feature = 50000 # get the 50000 most frequent words\n",
        "  max_len = 3000 # max number of words in a question to use\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=max_feature)\n",
        "  tokenizer.fit_on_texts(email)\n",
        "\n",
        "  eamil_seq = np.array(tokenizer.texts_to_sequences(email))\n",
        "\n",
        "  return pad_sequences(eamil_seq,maxlen=max_len)\n",
        "\n",
        "\n",
        "def Emails_Classifier(email):\n",
        "  email_pro = EmailsPreprocessor(email)\n",
        "  email_tok = Tokenizer_email(email_pro)\n",
        "  # load model's metadata\n",
        "  model = load_model('Email_classifier.h5')\n",
        "  # Model predict  a number from 0.0 to 1.0\n",
        "  y_pred = model.predict(email_tok)\n",
        "\n",
        "  if y_pred > 0.5 :\n",
        "    return 'Spam'\n",
        "\n",
        "  else :\n",
        "    return 'Ham'\n",
        "\n"
      ],
      "metadata": {
        "id": "CglxPj0ZG4ub"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email = ''\n",
        "\n",
        "Emails_Classifier(email)"
      ],
      "metadata": {
        "id": "oRtmaHJOK8yf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}