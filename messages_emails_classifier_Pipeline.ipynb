{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jZxeQ8FGnnD",
        "outputId": "5bf05912-8d4f-4b3a-aacb-2efdcac00705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.models import load_model\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the head of eamil\n",
        "def remove_header(email):\n",
        "    \"\"\"remove the header from an email\"\"\"\n",
        "    return email[email.index('\\n\\n'):]\n",
        "\n",
        "\n",
        "def remove_html_tags(input):\n",
        "    soup = BeautifulSoup(input, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "# replace URLs with oussama word and emails with boussaid\n",
        "def remove_hyperlink(word):\n",
        "    regex_links = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    word_without_links =  re.sub(regex_links,\"oussama\", word)\n",
        "    regex_email = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "    return re.sub(regex_email,\"boussaid\", word_without_links)\n",
        "\n",
        "\n",
        "# make word in lower case\n",
        "def to_lower(word):\n",
        "    return word.lower()\n",
        "\n",
        "\n",
        "# remove whitespaces\n",
        "def remove_whitespace(word):\n",
        "    return word.strip()\n",
        "\n",
        "\n",
        "def remove_digits(word):\n",
        "  '''This function removes all the numbers'''\n",
        "  return re.sub('\\d+', '', word)\n",
        "\n",
        "def remove_underscores(word):\n",
        "  '''This function removes all the underscores'''\n",
        "  return re.sub(r'_', '', word)\n",
        "\n",
        "\n",
        "def remove_special_characters(word):\n",
        "  '''This function removes all the special characters'''\n",
        "  return re.sub('\\W', ' ', word)\n",
        "\n",
        "# remove stop words\n",
        "stopwords_english = stopwords.words('english')\n",
        "def remove_stopwords(word,stopword_list=stopwords_english):\n",
        "  '''This function removes the stop words'''\n",
        "  word_list = word.split(\" \")\n",
        "  cleaned_txt = [w for w in word_list if not w in stopword_list]\n",
        "  cleaned_string = \" \".join(cleaned_txt)\n",
        "\n",
        "  return cleaned_string\n",
        "\n",
        "def EmailsPreprocessor(sentence):\n",
        "\n",
        "    Preprocessor_utils = [remove_header,\n",
        "                      remove_html_tags,\n",
        "                      to_lower,\n",
        "                      remove_hyperlink,\n",
        "                      remove_whitespace,\n",
        "                      remove_digits,\n",
        "                      remove_underscores,\n",
        "                      remove_stopwords,\n",
        "                      remove_special_characters]\n",
        "\n",
        "    for tool in Preprocessor_utils:\n",
        "        sentence = tool(sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def Tokenizer_email(email):\n",
        "  max_len = 3000 # max number of words in a question to use\n",
        "  # Load word_index from the saved JSON file\n",
        "  with open('word_index.json', 'r') as json_file:\n",
        "      loaded_word_index = json.load(json_file)\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.word_index = loaded_word_index\n",
        "  eamil_seq = np.array(tokenizer.texts_to_sequences([email]))\n",
        "  # print(eamil_seq)\n",
        "\n",
        "  return pad_sequences(eamil_seq,maxlen=max_len)\n",
        "\n",
        "\n",
        "def Emails_Classifier(email):\n",
        "  email_pro = EmailsPreprocessor(email)\n",
        "  print(email_pro)\n",
        "  email_tok = Tokenizer_email(email_pro)\n",
        "  print(email_tok)\n",
        "  # load model's metadata\n",
        "  model = load_model('Email_classifier.h5')\n",
        "  # Model predict  a number from 0.0 to 1.0\n",
        "  y_pred = model.predict(email_tok)\n",
        "\n",
        "  print(y_pred)\n",
        "\n",
        "  if y_pred[0] > 0.5 :\n",
        "    return 'Spam'\n",
        "\n",
        "  else :\n",
        "    return 'Ham'\n",
        "\n"
      ],
      "metadata": {
        "id": "CglxPj0ZG4ub"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get New Data"
      ],
      "metadata": {
        "id": "ZrF25xbPNqum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "DATASETS_DIR = 'datasets'\n",
        "MODELS_DIR = 'models'\n",
        "TAR_DIR = os.path.join(DATASETS_DIR, 'tar')\n",
        "\n",
        "EASY_HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2'\n",
        "SPAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2'\n",
        "HARD_HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2'\n",
        "SPAM2_URL = 'https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2'\n",
        "\n",
        "\n",
        "\n",
        "def download_dataset(url):\n",
        "    \"\"\"download and unzip data from a url into the specified path\"\"\"\n",
        "\n",
        "    # create directory if it doesn't exist\n",
        "    if not os.path.isdir(TAR_DIR):\n",
        "        os.makedirs(TAR_DIR)\n",
        "\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    tarpath = os.path.join(TAR_DIR, filename)\n",
        "\n",
        "    # download the tar file if it doesn't exist\n",
        "    try:\n",
        "        print(\"Downloading\", tarpath)\n",
        "        tarfile.open(tarpath)\n",
        "    except:\n",
        "        urlretrieve(url, tarpath)\n",
        "\n",
        "    with tarfile.open(tarpath) as tar:\n",
        "        dirname = os.path.join(DATASETS_DIR, tar.getnames()[0])\n",
        "        if os.path.isdir(dirname):\n",
        "            shutil.rmtree(dirname)\n",
        "        tar.extractall(path=DATASETS_DIR)\n",
        "\n",
        "        cmds_path = os.path.join(dirname, 'cmds')\n",
        "        if os.path.isfile(cmds_path):\n",
        "            os.remove(cmds_path)\n",
        "\n",
        "    return dirname\n",
        "\n",
        "\n",
        "def load_dataset(dirpath):\n",
        "    \"\"\"load emails from the specified directory\"\"\"\n",
        "\n",
        "    files = []\n",
        "    filepaths = glob.glob(dirpath + '/*')\n",
        "    for path in filepaths:\n",
        "        with open(path, 'rb') as f:\n",
        "            byte_content = f.read()\n",
        "            str_content = byte_content.decode('utf-8', errors='ignore')\n",
        "            files.append(str_content)\n",
        "\n",
        "    return files\n",
        "\n",
        "\n",
        "# download the data\n",
        "spam_dir = download_dataset(SPAM_URL)\n",
        "easy_ham_dir = download_dataset(EASY_HAM_URL)\n",
        "spam2 = download_dataset(SPAM2_URL)\n",
        "hard_ham_dir = download_dataset(HARD_HAM_URL)\n",
        "\n",
        "\n",
        "\n",
        "# load the datasets from datasets/tar/*\n",
        "spam = load_dataset(spam_dir)\n",
        "easy_ham = load_dataset(easy_ham_dir)\n",
        "spam2 = load_dataset(spam2)\n",
        "hard_ham = load_dataset(hard_ham_dir)\n",
        "\n",
        "\n",
        "print(\"Emails Ham :\", len(easy_ham))\n",
        "print(\"Emails Spam :\" ,len(spam))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWMFp41cNqfN",
        "outputId": "36a7630d-29a8-4357-aef4-e5254a0bd6dd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading datasets/tar/20021010_spam.tar.bz2\n",
            "Downloading datasets/tar/20030228_easy_ham.tar.bz2\n",
            "Downloading datasets/tar/20050311_spam_2.tar.bz2\n",
            "Downloading datasets/tar/20030228_hard_ham.tar.bz2\n",
            "Emails Ham : 2500\n",
            "Emails Spam : 501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "easy_ham[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "RDvHWqaCZ6oO",
        "outputId": "d57174c5-75f9-4696-f9c3-7c5160548c7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From rssfeeds@jmason.org  Thu Sep 26 16:34:02 2002\\nReturn-Path: <rssfeeds@spamassassin.taint.org>\\nDelivered-To: yyyy@localhost.spamassassin.taint.org\\nReceived: from localhost (jalapeno [127.0.0.1])\\n\\tby jmason.org (Postfix) with ESMTP id 6664D16F18\\n\\tfor <jm@localhost>; Thu, 26 Sep 2002 16:34:00 +0100 (IST)\\nReceived: from jalapeno [127.0.0.1]\\n\\tby localhost with IMAP (fetchmail-5.9.0)\\n\\tfor jm@localhost (single-drop); Thu, 26 Sep 2002 16:34:00 +0100 (IST)\\nReceived: from dogma.slashnull.org (localhost [127.0.0.1]) by\\n    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g8QFSTg24435 for\\n    <jm@jmason.org>; Thu, 26 Sep 2002 16:28:29 +0100\\nMessage-Id: <200209261528.g8QFSTg24435@dogma.slashnull.org>\\nTo: yyyy@spamassassin.taint.org\\nFrom: joelonsoftware <rssfeeds@spamassassin.taint.org>\\nSubject: We\\'re trying to decide if FogBUGZ 3.0 should support custom\\n    fields. Histor\\nDate: Thu, 26 Sep 2002 15:28:28 -0000\\nContent-Type: text/plain; encoding=utf-8\\n\\nURL: http://www.joelonsoftware.com/news/20020912.html\\nDate: Not supplied\\n\\nWe\\'re trying to decide if FogBUGZ[1] 3.0 should support custom fields. \\nHistorically, I am opposed to custom fields in principle, because they get \\nabused. People add so many fields to their bug databases to capture everything \\nthey think might be important that entering a bug is like applying to Harvard. \\nEnd result: people don\\'t enter bugs, which is much, much worse than not \\ncapturing all that information. You can always \"page fault\" to get the \\ninformation if the original report forgot it. Rather than having a field in \\nevery bug where you enter the version numbers of every DLL on your machine \\n(this is an actual customer request), information which is likely to be \\nrelevant only for a tiny percentage of bugs, why not just have the \\nprogrammer-assignee look at the bug first, and if they think it might be \\ndll-version-related, only _then_ bounce the bug back to the originator asking \\nfor the DLL info? Similarly, it\\'s always tempting to add a field in which you \\nask for the OS version in which the bug occurred. This sounds logical, but \\ntrust me: adding fields like this is guaranteed to do one thing and one thing \\nonly: reduce the number of bug reports that get into the system in the first \\nplace. Only a small percentage of bugs are really OS dependant and you can \\nalways include that info in the text description of the bug if it happens to be \\nrelevant. (But then how do you search for, say, all bugs which only happen on \\nWindows 98SE? Aha! You can\\'t. Ever. Even with the custom field. Because not \\nevery bug has been regressed on every version of every operating system, so \\nthis search doesn\\'t make sense in the first place. The info wasn\\'t captured. Do \\na full text search for 98SE and you\\'ll find some of them. Life is imperfect.) \\n\\nLife would be more perfect if every bug report included megabytes of \\ninformation -- a complete dump of every byte on the hard drive and in RAM on \\nthe computer in question and while you\\'re at it, a photograph of the tester\\'s \\nworkspace. But the goal of a bug tracking database is to _keep track of bugs_, \\nwhich, all else being equal, takes priority over making it easy to find them. I \\nhave heard countless stories of development teams where the bug tracking \\npackage was so high-ceremony that people were afraid to enter bugs in the \\nsystem, because they didn\\'t know what all those fields were. The _real_ \\nbug-\"tracking\" happened in email, post-its, and hallway conversations. Great. \\n\\nA pretty common question we get on the customer service line is, \"does \\nFogBUGZ support custom fields?\" Rather than giving our usual answer (\"no. on \\npurpose.\") over the last few weeks I\\'ve been saying, \"can you please tell me \\nwhat fields you would need? We\\'re trying to decide whether to implement that \\nfeature in 3.0 and we want to know why people need it.\" The interesting thing \\nis, almost all of the fields people ask for _are already in FogBUGZ,_ and the \\nother ones, in my educated opinion, shouldn\\'t be fields. And in fact, our \\nexisting customers are certainly happy without custom fields. One of our \\nbiggest site licenses was sold to a semiconductor company, and I myself wanted \\nto add a custom field for them to keep track of versions of the circuit design, \\nbut I didn\\'t, and they never needed it (even though they had been keeping track \\nof it with the old bug tracking package which had custom fields), and they are \\nhappy and keep buying more site licenses. \\n\\nBut the dilemma for us is that many customers are evaluating bug tracking \\nsoftware and they consider the lack of custom fields to be a major weakness in \\nour product. \"Gosh, even Filemaker has custom fields.\" Righto. It\\'s true. And \\nwho am I to tell my customers they are wrong? One person who I was talking to \\nyesterday would have used a custom field for something that we already have a \\nbuilt-in field for. This would have made their database confusing and \\ninconsistent and would have definitely caused more problems than it solved. But \\nit\\'s still rude of me to tell customers that we don\\'t have that feature _for \\ntheir own good_, even though it usually is, and we\\'re losing some sales because \\nof it. \\n\\nSigh. I guess we could have a custom fields feature but hide it and make it so \\nhard to use that people don\\'t use it. At least we won\\'t lose any sales :)\\n\\n[1] http://www.fogcreek.com/FogBUGZ\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Emails_Classifier(spam2[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "oRtmaHJOK8yf",
        "outputId": "f31f3fdc-0576-4656-bbb3-df31c45a021a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi part message mime format          nextpartace db content type  text plain   charset  windows   content transfer encoding  bit   ann arbor annuity exchange    giveaway    think annuities   think ann arbor  just short list many companies represent         fill form free entry    giveaway    name      e mail      phone      fax      city     state                      agent use only  employees family members ann arbor annuity exchange  subsidiaries ineligible   we want anybody receive mailing wish to receive them  professional communication sent insurance professionals  removed mailing list  reply to this message  instead  go here  oussama legal notice             nextpartace db content type  text html   charset  iso    content transfer encoding  quoted printable       giveaway                                                          just short list many companies   represent                                                                                                                                                                                      fill   out form free entry    giveaway                               name                                                               e mail                                                               phone                                                               fax                                                               city                                  state                                                                                                                                                                               agent use only  employees family members ann   arbor annuity exchange                  subsidiaries ineligible  we want anybody receive mailing   not wish                  receive them  professional communication   sent to                  insurance professionals  removed mailing   list                   reply message  instead  go here                  oussama legal   notice                                 nextpartace db  \n",
            "[[    0     0     0 ...   662 14350  1693]]\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "[[0.99092877]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}